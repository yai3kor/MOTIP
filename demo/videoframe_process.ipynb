{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25c3e69755e855a7",
   "metadata": {},
   "source": [
    "# Video Demo\n",
    "Here, we provide a demo on how to use our model to process a video and visualize the tracking results.\n",
    "\n",
    "We selected an open hpop dance video from the internet to demonstrate our demo. You can also choose other custom videos. **Please note that it is crucial to select the appropriate trained MOTIP weights and configuration for different tracking scenarios.**\n",
    "\n",
    "We process the video on NVIDIA RTX 3080Ti, achieving a nearly real-time tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dadffc91229559",
   "metadata": {},
   "source": [
    "### System Environment\n",
    "1. Modify the root path to the project path.\n",
    "2. Make sure you have a cuda device available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T04:43:01.735691Z",
     "start_time": "2025-04-06T04:43:00.935612Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current root path is set to /media/RTCIN9TBA/Interns/RDT2/yai3kor/workspace/MOTIP\n",
      "Hello! Welcome to use the video process demo. Your torch version is 2.3.0 and CUDA is available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "\n",
    "current_file_path = os.path.abspath(\"/media/RTCIN9TBA/Interns/RDT2/yai3kor/workspace/MOTIP/demo\")\n",
    "parent_dir = os.path.dirname(current_file_path)\n",
    "sys.path.append(parent_dir)\n",
    "os.chdir(parent_dir)\n",
    "print(f\"Current root path is set to {parent_dir}\")\n",
    "\n",
    "torch_version = torch.__version__\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "if not cuda_available:\n",
    "    raise RuntimeError(\"CUDA is not available\")\n",
    "\n",
    "print(f\"Hello! Welcome to use the video process demo. Your torch version is {torch_version} and CUDA is available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f513017d6a31533",
   "metadata": {},
   "source": [
    "### Prepare your video (.mp4 for example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40cbc65feda44f45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T04:43:05.010231Z",
     "start_time": "2025-04-06T04:43:05.006827Z"
    }
   },
   "outputs": [],
   "source": [
    "#os.makedirs(\"./outputs/video_process_demo/\", exist_ok=True)\n",
    "video_path = os.path.join(\"/media/RTCIN9TBA/Interns/RDT2/yai3kor/workspace/sam2/notebooks/videos\", f\"bedroom.mp4\")\n",
    "output_path = os.path.join(\"/media/RTCIN9TBA/Interns/RDT2/yai3kor/workspace/output/motip/bosch1_hungarian\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dd461e8a64cf98",
   "metadata": {},
   "source": [
    "### Build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e177068ce0247419",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T04:43:19.445797Z",
     "start_time": "2025-04-06T04:43:17.795812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built successfully.\n"
     ]
    }
   ],
   "source": [
    "from utils.misc import yaml_to_dict\n",
    "\n",
    "\n",
    "config_path = \"configs/r50_deformable_detr_motip_dancetrack.yaml\"\n",
    "checkpoint_path = \"/media/RTCIN9TBA/Interns/RDT2/yai3kor/workspace/r50_deformable_detr_motip_dancetrack.pth\"\n",
    "config = yaml_to_dict(config_path)\n",
    "dtype = torch.float16       # torch.float32 or torch.float16, we select float16 for faster inference\n",
    "\n",
    "\n",
    "from models.motip import build as build_model\n",
    "from models.misc import load_checkpoint\n",
    "from models.runtime_tracker import RuntimeTracker\n",
    "model, _ = build_model(config)\n",
    "# Load the model weights\n",
    "load_checkpoint(model, checkpoint_path)\n",
    "model.eval()\n",
    "model = model.cuda()\n",
    "if dtype == torch.float16:\n",
    "    model.half()\n",
    "\n",
    "print(\"Model built successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d412dbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/RTCIN9TBA/Interns/RDT2/yai3kor/workspace/MOTIP\n"
     ]
    }
   ],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba421cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters().__next__().dtype)  # shows model's dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda99259fbf3a3df",
   "metadata": {},
   "source": [
    "### Process the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1502b31dd6aedf8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T04:46:38.360921Z",
     "start_time": "2025-04-06T04:43:22.912949Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking objects: 100%|██████████| 927/927 [06:36<00:00,  2.34frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing all frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import functional as F\n",
    "from utils.nested_tensor import nested_tensor_from_tensor_list\n",
    "from demo.colormap import get_color\n",
    "\n",
    "\n",
    "def simple_transform(image, max_shorter, max_longer, image_dtype):\n",
    "    image = F.to_tensor(image)\n",
    "    image = F.resize(image, size=max_shorter, max_size=max_longer)\n",
    "    image = F.normalize(image, mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "    if image_dtype != torch.float32:\n",
    "        image = image.to(image_dtype)\n",
    "    return image.cuda()\n",
    "\n",
    "\n",
    "# === INPUT CONFIGURATION ===\n",
    "frames_dir = \"/media/RTCIN9TBA/Interns/RDT2/yai3kor/workspace/data/BOSCH/LB-UH_103_20181116_145538_005\"         # directory with frames\n",
    "dtype = torch.float16                   # or torch.float16\n",
    "\n",
    "# === FRAME COLLECTION ===\n",
    "frame_files = sorted([\n",
    "    os.path.join(frames_dir, f)\n",
    "    for f in os.listdir(frames_dir)\n",
    "    if f.lower().endswith((\".jpg\",\".png\"))\n",
    "])\n",
    "\n",
    "if not frame_files:\n",
    "    raise RuntimeError(f\"No frames found in: {frames_dir}\")\n",
    "\n",
    "# === INIT TRACKER ===\n",
    "sample_frame = cv2.imread(frame_files[0])\n",
    "height, width = sample_frame.shape[:2]\n",
    "\n",
    "runtime_tracker = RuntimeTracker(\n",
    "    model=model,\n",
    "    sequence_hw=(height, width),\n",
    "    assignment_protocol=\"hungarian\",\n",
    "    miss_tolerance=30,\n",
    "    det_thresh=0.4,\n",
    "    newborn_thresh=0.5,\n",
    "    id_thresh=0.6,\n",
    "    dtype=dtype,\n",
    ")\n",
    "\n",
    "# === PROCESS FRAMES ===\n",
    "for frame_path in tqdm(frame_files, desc=\"Tracking objects\", unit=\"frame\"):\n",
    "    frame = cv2.imread(frame_path)\n",
    "    if frame is None:\n",
    "        print(f\"Could not read frame: {frame_path}\")\n",
    "        continue\n",
    "\n",
    "    # Prepare tensor input\n",
    "    frame_tensor = simple_transform(frame, max_shorter=800, max_longer=1440, image_dtype=dtype)\n",
    "    frame_tensor = nested_tensor_from_tensor_list([frame_tensor])\n",
    "\n",
    "    # Run tracker\n",
    "    runtime_tracker.update(frame_tensor)\n",
    "    with torch.no_grad():\n",
    "        track_results = runtime_tracker.get_track_results()\n",
    "\n",
    "    # Annotate frame (in memory only)\n",
    "    for bbox, obj_id in zip(track_results[\"bbox\"], track_results[\"id\"]):\n",
    "        x, y, w, h = map(int, bbox)\n",
    "        color = get_color(obj_id, rgb=False, use_int=True)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "        cv2.putText(frame, f\"ID: {obj_id}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    # You can visualize (optional)\n",
    "    # cv2.imshow(\"Tracked\", frame)\n",
    "    # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    #     break\n",
    "\n",
    "# cv2.destroyAllWindows()\n",
    "    frame_name = os.path.basename(frame_path)\n",
    "    save_path = os.path.join(output_path, frame_name)\n",
    "    cv2.imwrite(save_path, frame)\n",
    "print(\"Done processing all frames.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c9ecf003296290d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160 3840\n"
     ]
    }
   ],
   "source": [
    "print(height,width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f83986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motip_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
