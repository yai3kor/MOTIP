{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25c3e69755e855a7",
   "metadata": {},
   "source": [
    "# Video Demo\n",
    "Here, we provide a demo on how to use our model to process a video and visualize the tracking results.\n",
    "\n",
    "We selected an open hpop dance video from the internet to demonstrate our demo. You can also choose other custom videos. **Please note that it is crucial to select the appropriate trained MOTIP weights and configuration for different tracking scenarios.**\n",
    "\n",
    "We process the video on NVIDIA RTX 3080Ti, achieving a nearly real-time tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dadffc91229559",
   "metadata": {},
   "source": [
    "### System Environment\n",
    "1. Modify the root path to the project path.\n",
    "2. Make sure you have a cuda device available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T04:43:01.735691Z",
     "start_time": "2025-04-06T04:43:00.935612Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current root path is set to /media/RTCIN9TBA/Interns/RDT2/yai3kor/workspace/MOTIP\n",
      "Hello! Welcome to use the video process demo. Your torch version is 2.3.0 and CUDA is available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "\n",
    "current_file_path = os.path.abspath(\"/media/RTCIN9TBA/Interns/RDT2/yai3kor/workspace/MOTIP/demo\")\n",
    "parent_dir = os.path.dirname(current_file_path)\n",
    "sys.path.append(parent_dir)\n",
    "os.chdir(parent_dir)\n",
    "print(f\"Current root path is set to {parent_dir}\")\n",
    "\n",
    "torch_version = torch.__version__\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "if not cuda_available:\n",
    "    raise RuntimeError(\"CUDA is not available\")\n",
    "\n",
    "print(f\"Hello! Welcome to use the video process demo. Your torch version is {torch_version} and CUDA is available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f513017d6a31533",
   "metadata": {},
   "source": [
    "### Prepare your video (.mp4 for example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40cbc65feda44f45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T04:43:05.010231Z",
     "start_time": "2025-04-06T04:43:05.006827Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"./outputs/video_process_demo/\", exist_ok=True)\n",
    "video_path = os.path.join(\"/media/RTCIN9TBA/Interns/RDT2/yai3kor/workspace/sam2/notebooks/videos\", f\"bedroom.mp4\")\n",
    "output_path = os.path.join(\"./outputs/video_process_demo/\", f\"bedroom_tracking.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770162a8ffb32c70",
   "metadata": {},
   "source": [
    "#### [Optional] Download a video from Bilibili if you don't have a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32c5b97fdba544ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T04:43:11.939185Z",
     "start_time": "2025-04-06T04:43:08.950120Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33myou-get: You will need login cookies for 720p formats or above. (use --cookies to load cookies.txt.)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site:                Bilibili\n",
      "title:               izna《SIGN》练习室舞蹈(Fix ver.)\n",
      "stream:\n",
      "    - format:        \u001b[7mdash-flv480-AVC\u001b[0m\n",
      "      container:     mp4\n",
      "      quality:       清晰 480P avc1.640033\n",
      "      size:          13.5 MiB (14166376 bytes)\n",
      "    # download-with: \u001b[4myou-get --format=dash-flv480-AVC [URL]\u001b[0m\n",
      "\n",
      "Downloading izna《SIGN》练习室舞蹈(Fix ver.).mp4 ...\n",
      " 100% ( 13.5/ 13.5MB) ├████████████████████████████████████████┤[2/2]  253 MB/s\n",
      "Merging video parts... Merged into izna《SIGN》练习室舞蹈(Fix ver.).mp4\n",
      "\n",
      "Downloading izna《SIGN》练习室舞蹈(Fix ver.).cmt.xml ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "video_url = \"https://www.bilibili.com/video/BV19mZ2YzERT/\"\n",
    "video_dir = os.path.join(\"./outputs/video_process_demo/\", f\"hpop_dancers\")\n",
    "\n",
    "os.system(f\"you-get -o {video_dir} {video_url}\")\n",
    "files = os.listdir(video_dir)\n",
    "# Search the .mp4 file, change name to \"hpop_dancers.mp4\", move to outputs/video_process_demo/\n",
    "for file in files:\n",
    "    if file.endswith(\".mp4\"):\n",
    "        os.rename(os.path.join(video_dir, file), video_path)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05dc2cb4e8cd581",
   "metadata": {},
   "source": [
    "#### [Optional] Display the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8c48fda8f3c9f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video controls  >\n",
       " <source src=\"data:None;base64,/media/RTCIN9TBA/Interns/RDT2/yai3kor/workspace/sam2/notebooks/videos/bedroom.mp4/bedroom.mp4\" type=\"None\">\n",
       " Your browser does not support the video tag.\n",
       " </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(video_path, embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dd461e8a64cf98",
   "metadata": {},
   "source": [
    "### Build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e177068ce0247419",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T04:43:19.445797Z",
     "start_time": "2025-04-06T04:43:17.795812Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /media/RTCIN7TBDriveB/home/yai3kor/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:32<00:00, 3.11MB/s]\n",
      "/media/RTCIN9TBA/Interns/RDT2/yai3kor/.cache/motip_env/lib/python3.10/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608935911/work/aten/src/ATen/native/TensorShape.cpp:3587.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built successfully.\n"
     ]
    }
   ],
   "source": [
    "from utils.misc import yaml_to_dict\n",
    "\n",
    "\n",
    "config_path = \"configs/r50_deformable_detr_motip_dancetrack.yaml\"\n",
    "checkpoint_path = \"/media/RTCIN9TBA/Interns/RDT2/yai3kor/workspace/r50_deformable_detr_motip_dancetrack.pth\"\n",
    "config = yaml_to_dict(config_path)\n",
    "dtype = torch.float16       # torch.float32 or torch.float16, we select float16 for faster inference\n",
    "\n",
    "\n",
    "from models.motip import build as build_model\n",
    "from models.misc import load_checkpoint\n",
    "from models.runtime_tracker import RuntimeTracker\n",
    "model, _ = build_model(config)\n",
    "# Load the model weights\n",
    "load_checkpoint(model, checkpoint_path)\n",
    "model.eval()\n",
    "model = model.cuda()\n",
    "if dtype == torch.float16:\n",
    "    model.half()\n",
    "\n",
    "print(\"Model built successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d412dbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/RTCIN9TBA/Interns/RDT2/yai3kor/workspace/MOTIP\n"
     ]
    }
   ],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda99259fbf3a3df",
   "metadata": {},
   "source": [
    "### Process the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1502b31dd6aedf8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T04:46:38.360921Z",
     "start_time": "2025-04-06T04:43:22.912949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video /media/RTCIN9TBA/Interns/RDT2/yai3kor/workspace/sam2/notebooks/videos/bedroom.mp4 seems OK. It has 30.0 fps, 960 width and 540 height.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:   0%|          | 0/200 [00:00<?, ?frame/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video: 100%|██████████| 200/200 [00:18<00:00, 10.91frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video processing completed. The output video is saved to ./outputs/video_process_demo/bedroom_tracking.mp4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from utils.nested_tensor import nested_tensor_from_tensor_list\n",
    "from tqdm import tqdm\n",
    "from demo.colormap import get_color\n",
    "\n",
    "\n",
    "def simple_transform(\n",
    "        image, max_shorter, max_longer, image_dtype,\n",
    "):\n",
    "    from torchvision.transforms import functional as F\n",
    "\n",
    "    image = F.to_tensor(image)\n",
    "    image = F.resize(image, size=max_shorter, max_size=max_longer)\n",
    "    image = F.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    if image_dtype != torch.float32:\n",
    "        image = image.to(image_dtype)\n",
    "    return image.cuda()\n",
    "\n",
    "\n",
    "video_cap = cv2.VideoCapture(video_path)\n",
    "if not video_cap.isOpened():\n",
    "    raise RuntimeError(f\"Failed to open video file: {video_path}\")\n",
    "# Get video properties\n",
    "fps = video_cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "length = int(video_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(f\"The video {video_path} seems OK. It has {fps} fps, {width} width and {height} height.\")\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "video_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "runtime_tracker = RuntimeTracker(\n",
    "    model=model,\n",
    "    sequence_hw=(height, width),\n",
    "    assignment_protocol=\"object-max\",\n",
    "    miss_tolerance=30,\n",
    "    det_thresh=0.5,\n",
    "    newborn_thresh=0.5,\n",
    "    id_thresh=0.2,\n",
    "    dtype=dtype,\n",
    ")\n",
    "\n",
    "for frame_idx in tqdm(range(length), desc=\"Processing video\", unit=\"frame\"):\n",
    "    ret, frame = video_cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to a tensor\n",
    "    frame_tensor = simple_transform(frame, max_shorter=800, max_longer=1440, image_dtype=dtype)\n",
    "    frame_tensor = nested_tensor_from_tensor_list([frame_tensor])\n",
    "\n",
    "    # Run the tracker on the frame\n",
    "    runtime_tracker.update(frame_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        track_results = runtime_tracker.get_track_results()\n",
    "\n",
    "    for bbox, obj_id in zip(track_results[\"bbox\"], track_results[\"id\"]):\n",
    "        x, y, w, h = map(int, bbox)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), get_color(obj_id, rgb=False, use_int=True), 2)\n",
    "        cv2.putText(frame, f\"ID: {obj_id}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, get_color(obj_id, rgb=False, use_int=True), 2)\n",
    "\n",
    "    video_writer.write(frame)\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "video_cap.release()\n",
    "video_writer.release()\n",
    "\n",
    "print(f\"Video processing completed. The output video is saved to {output_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9ecf003296290d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motip_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
